{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62cb1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4605093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca5ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reynolds numbers from 10 to 90 are used for training \n",
    "# The model is evaluated on the last 10 reynolds numbers from 91 to 100\n",
    "\n",
    "outputs = np.zeros((90-10+1, 2223))\n",
    "for Re in range(10,91,1):\n",
    "    outputs[Re-10] = np.loadtxt('pinns/pretrained/weights/weights_'+str(Re)+'.txt')\n",
    "    \n",
    "# Inputs represent the reynolds numbers used for training\n",
    "# Outputs are the outputs of the hypernetwork which are the weights of the PINN\n",
    "inputs = np.arange(10,91,1)\n",
    "inputs = np.reshape(inputs, (inputs.shape[0],1))\n",
    "inputs = Variable(torch.from_numpy(inputs).float(), requires_grad=False).to(device)\n",
    "outputs = Variable(torch.from_numpy(outputs).float(), requires_grad=False).to(device)\n",
    "#print(\"inputs\", inputs.shape)\n",
    "#print(\"outputs\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96dd2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HyperNetwork, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(nn.Linear(1,512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(512,512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(512,256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(256,256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(256,128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128,2223))    \n",
    "        \n",
    "    def forward(self, Re):        \n",
    "        weights = self.layers(Re)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7f6b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "hnet = HyperNetwork()\n",
    "hnet = hnet.to(device)\n",
    "mse_cost_function = nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(hnet.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=15000, gamma=0.1, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4585068b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, LR: 1.0000e-03, Loss: 3.4437e-01\n",
      "Epoch 1000, LR: 1.0000e-03, Loss: 1.1520e-01\n",
      "Epoch 2000, LR: 1.0000e-03, Loss: 1.1147e-01\n",
      "Epoch 3000, LR: 1.0000e-03, Loss: 9.6513e-02\n",
      "Epoch 4000, LR: 1.0000e-03, Loss: 1.0591e-01\n",
      "Epoch 5000, LR: 1.0000e-03, Loss: 9.4622e-02\n",
      "Epoch 6000, LR: 1.0000e-03, Loss: 9.0187e-02\n",
      "Epoch 7000, LR: 1.0000e-03, Loss: 8.1966e-02\n",
      "Epoch 8000, LR: 1.0000e-03, Loss: 1.0534e-01\n",
      "Epoch 9000, LR: 1.0000e-03, Loss: 9.4605e-02\n",
      "Epoch 10000, LR: 1.0000e-03, Loss: 8.8132e-02\n",
      "Epoch 11000, LR: 1.0000e-03, Loss: 9.5286e-02\n",
      "Epoch 12000, LR: 1.0000e-03, Loss: 8.4852e-02\n",
      "Epoch 13000, LR: 1.0000e-03, Loss: 9.3628e-02\n",
      "Epoch 14000, LR: 1.0000e-03, Loss: 9.0128e-02\n",
      "Epoch 15000, LR: 1.0000e-04, Loss: 8.7932e-02\n",
      "Epoch 16000, LR: 1.0000e-04, Loss: 8.6085e-02\n",
      "Epoch 17000, LR: 1.0000e-04, Loss: 8.5473e-02\n",
      "Epoch 18000, LR: 1.0000e-04, Loss: 8.4756e-02\n",
      "Epoch 19000, LR: 1.0000e-04, Loss: 8.3763e-02\n",
      "Epoch 20000, LR: 1.0000e-04, Loss: 8.2747e-02\n",
      "Epoch 21000, LR: 1.0000e-04, Loss: 8.1869e-02\n",
      "Epoch 22000, LR: 1.0000e-04, Loss: 8.1174e-02\n",
      "Epoch 23000, LR: 1.0000e-04, Loss: 8.0247e-02\n",
      "Epoch 24000, LR: 1.0000e-04, Loss: 7.9665e-02\n",
      "Epoch 25000, LR: 1.0000e-04, Loss: 7.9184e-02\n",
      "Epoch 26000, LR: 1.0000e-04, Loss: 7.8776e-02\n",
      "Epoch 27000, LR: 1.0000e-04, Loss: 7.8343e-02\n",
      "Epoch 28000, LR: 1.0000e-04, Loss: 7.7972e-02\n",
      "Epoch 29000, LR: 1.0000e-04, Loss: 7.7531e-02\n",
      "Epoch 30000, LR: 1.0000e-05, Loss: 7.7300e-02\n",
      "Epoch 31000, LR: 1.0000e-05, Loss: 7.7102e-02\n",
      "Epoch 32000, LR: 1.0000e-05, Loss: 7.7036e-02\n",
      "Epoch 33000, LR: 1.0000e-05, Loss: 7.6946e-02\n",
      "Epoch 34000, LR: 1.0000e-05, Loss: 7.6849e-02\n",
      "Epoch 35000, LR: 1.0000e-05, Loss: 7.6752e-02\n",
      "Epoch 36000, LR: 1.0000e-05, Loss: 7.6640e-02\n",
      "Epoch 37000, LR: 1.0000e-05, Loss: 7.6534e-02\n",
      "Epoch 38000, LR: 1.0000e-05, Loss: 7.6421e-02\n",
      "Epoch 39000, LR: 1.0000e-05, Loss: 7.6321e-02\n",
      "Epoch 40000, LR: 1.0000e-05, Loss: 7.6221e-02\n",
      "Epoch 41000, LR: 1.0000e-05, Loss: 7.6117e-02\n",
      "Epoch 42000, LR: 1.0000e-05, Loss: 7.6019e-02\n",
      "Epoch 43000, LR: 1.0000e-05, Loss: 7.5922e-02\n",
      "Epoch 44000, LR: 1.0000e-05, Loss: 7.5841e-02\n",
      "Epoch 45000, LR: 1.0000e-06, Loss: 7.5738e-02\n",
      "Epoch 46000, LR: 1.0000e-06, Loss: 7.5714e-02\n",
      "Epoch 47000, LR: 1.0000e-06, Loss: 7.5699e-02\n",
      "Epoch 48000, LR: 1.0000e-06, Loss: 7.5681e-02\n",
      "Epoch 49000, LR: 1.0000e-06, Loss: 7.5662e-02\n",
      "Epoch 50000, LR: 1.0000e-06, Loss: 7.5641e-02\n",
      "Epoch 51000, LR: 1.0000e-06, Loss: 7.5620e-02\n",
      "Epoch 52000, LR: 1.0000e-06, Loss: 7.5599e-02\n",
      "Epoch 53000, LR: 1.0000e-06, Loss: 7.5579e-02\n",
      "Epoch 54000, LR: 1.0000e-06, Loss: 7.5558e-02\n",
      "Epoch 55000, LR: 1.0000e-06, Loss: 7.5538e-02\n",
      "Epoch 56000, LR: 1.0000e-06, Loss: 7.5518e-02\n",
      "Epoch 57000, LR: 1.0000e-06, Loss: 7.5499e-02\n",
      "Epoch 58000, LR: 1.0000e-06, Loss: 7.5479e-02\n",
      "Epoch 59000, LR: 1.0000e-06, Loss: 7.5459e-02\n",
      "Epoch 60000, LR: 1.0000e-07, Loss: 7.5439e-02\n",
      "Epoch 61000, LR: 1.0000e-07, Loss: 7.5437e-02\n",
      "Epoch 62000, LR: 1.0000e-07, Loss: 7.5435e-02\n",
      "Epoch 63000, LR: 1.0000e-07, Loss: 7.5432e-02\n",
      "Epoch 64000, LR: 1.0000e-07, Loss: 7.5430e-02\n",
      "Epoch 65000, LR: 1.0000e-07, Loss: 7.5427e-02\n",
      "Epoch 66000, LR: 1.0000e-07, Loss: 7.5424e-02\n",
      "Epoch 67000, LR: 1.0000e-07, Loss: 7.5422e-02\n",
      "Epoch 68000, LR: 1.0000e-07, Loss: 7.5419e-02\n",
      "Epoch 69000, LR: 1.0000e-07, Loss: 7.5416e-02\n",
      "Epoch 70000, LR: 1.0000e-07, Loss: 7.5414e-02\n",
      "Epoch 71000, LR: 1.0000e-07, Loss: 7.5411e-02\n",
      "Epoch 72000, LR: 1.0000e-07, Loss: 7.5409e-02\n",
      "Epoch 73000, LR: 1.0000e-07, Loss: 7.5406e-02\n",
      "Epoch 74000, LR: 1.0000e-07, Loss: 7.5403e-02\n",
      "Epoch 75000, LR: 1.0000e-08, Loss: 7.5401e-02\n",
      "Epoch 76000, LR: 1.0000e-08, Loss: 7.5400e-02\n",
      "Epoch 77000, LR: 1.0000e-08, Loss: 7.5400e-02\n",
      "Epoch 78000, LR: 1.0000e-08, Loss: 7.5400e-02\n",
      "Epoch 79000, LR: 1.0000e-08, Loss: 7.5400e-02\n",
      "Epoch 80000, LR: 1.0000e-08, Loss: 7.5400e-02\n",
      "Epoch 81000, LR: 1.0000e-08, Loss: 7.5399e-02\n",
      "Epoch 82000, LR: 1.0000e-08, Loss: 7.5399e-02\n",
      "Epoch 83000, LR: 1.0000e-08, Loss: 7.5399e-02\n",
      "Epoch 84000, LR: 1.0000e-08, Loss: 7.5399e-02\n",
      "Epoch 85000, LR: 1.0000e-08, Loss: 7.5398e-02\n",
      "Epoch 86000, LR: 1.0000e-08, Loss: 7.5398e-02\n",
      "Epoch 87000, LR: 1.0000e-08, Loss: 7.5398e-02\n",
      "Epoch 88000, LR: 1.0000e-08, Loss: 7.5398e-02\n",
      "Epoch 89000, LR: 1.0000e-08, Loss: 7.5398e-02\n",
      "Epoch 90000, LR: 1.0000e-09, Loss: 7.5397e-02\n",
      "Epoch 91000, LR: 1.0000e-09, Loss: 7.5397e-02\n",
      "Epoch 92000, LR: 1.0000e-09, Loss: 7.5397e-02\n",
      "Epoch 93000, LR: 1.0000e-09, Loss: 7.5397e-02\n",
      "Epoch 94000, LR: 1.0000e-09, Loss: 7.5397e-02\n",
      "Epoch 95000, LR: 1.0000e-09, Loss: 7.5397e-02\n",
      "Epoch 96000, LR: 1.0000e-09, Loss: 7.5397e-02\n",
      "Epoch 97000, LR: 1.0000e-09, Loss: 7.5397e-02\n",
      "Epoch 98000, LR: 1.0000e-09, Loss: 7.5397e-02\n",
      "Epoch 99000, LR: 1.0000e-09, Loss: 7.5397e-02\n"
     ]
    }
   ],
   "source": [
    "iterations = 100000\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    hnet_out = hnet.forward(inputs)\n",
    "    Loss = mse_cost_function(hnet_out, outputs)\n",
    "    Loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "        if epoch%1000 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print('Epoch %d, LR: %.4e, Loss: %.4e' % (epoch, current_lr, Loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "049ff99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = np.zeros((10, 2223))\n",
    "# for Re in range(91,101,1):\n",
    "#     outputs[Re-91] = np.loadtxt('pinns/pretrained/weights/weights_'+str(Re)+'.txt')\n",
    "    \n",
    "# inputs = np.arange(91,101,1)\n",
    "# inputs = np.reshape(inputs, (inputs.shape[0],1))\n",
    "# inputs = Variable(torch.from_numpy(inputs).float(), requires_grad=False).to(device)\n",
    "# outputs = Variable(torch.from_numpy(outputs).float(), requires_grad=False).to(device)\n",
    "# pred_outputs = hnet.forward(inputs)\n",
    "# L2_error = torch.linalg.norm(outputs-pred_outputs,2)/torch.linalg.norm(outputs,2)\n",
    "# print('Relative L2 Error: %e \\n' % (L2_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4bffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "    \n",
    "    def forward(self, weights, x):\n",
    "        h = torch.tanh(F.linear(x, weight = weights[0, :40].view(20, 2), bias = weights[0, 40:60].view(20)))\n",
    "        i = 60\n",
    "        for _ in range(5):\n",
    "            h = torch.tanh(F.linear(h, weight = weights[0, i:i+400].view(20, 20), bias = weights[0, i+400:i+420].view(20)))\n",
    "            i += 420\n",
    "        h = torch.linear(h, weight = weights[0, i:i+60].view(3, 20), bias = weights[0, i+20:i+23].view(3))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea0e6bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re:  91\n",
      "Mean L2 Error: 7.2094e+01\n",
      "Relative L2 Error: 1.2028e+00 \n",
      "\n",
      "Re:  92\n",
      "Mean L2 Error: 8.4092e+01\n",
      "Relative L2 Error: 1.4028e+00 \n",
      "\n",
      "Re:  93\n",
      "Mean L2 Error: 9.4236e+01\n",
      "Relative L2 Error: 1.5719e+00 \n",
      "\n",
      "Re:  94\n",
      "Mean L2 Error: 1.0173e+02\n",
      "Relative L2 Error: 1.6966e+00 \n",
      "\n",
      "Re:  95\n",
      "Mean L2 Error: 1.0922e+02\n",
      "Relative L2 Error: 1.8214e+00 \n",
      "\n",
      "Re:  96\n",
      "Mean L2 Error: 1.1821e+02\n",
      "Relative L2 Error: 1.9711e+00 \n",
      "\n",
      "Re:  97\n",
      "Mean L2 Error: 1.2851e+02\n",
      "Relative L2 Error: 2.1426e+00 \n",
      "\n",
      "Re:  98\n",
      "Mean L2 Error: 1.4022e+02\n",
      "Relative L2 Error: 2.3376e+00 \n",
      "\n",
      "Re:  99\n",
      "Mean L2 Error: 1.4985e+02\n",
      "Relative L2 Error: 2.4978e+00 \n",
      "\n",
      "Re:  100\n",
      "Mean L2 Error: 1.5752e+02\n",
      "Relative L2 Error: 2.6255e+00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pinn = PINN()\n",
    "\n",
    "f = h5py.File('data/colocation_points.h5', 'r')\n",
    "X_col = f['X_col']\n",
    "Y_col = f['Y_col']\n",
    "X_col = np.asarray(X_col)\n",
    "Y_col = np.asarray(Y_col)\n",
    "X_col = Variable(torch.from_numpy(X_col).float(), requires_grad=True).to(device)\n",
    "Y_col = Variable(torch.from_numpy(Y_col).float(), requires_grad=True).to(device)\n",
    "inputs = torch.cat([X_col, Y_col], axis=1)\n",
    "f.close()\n",
    "\n",
    "for Re in range(91,101,1):\n",
    "    Re_tensor = torch.tensor(Re, dtype=torch.float32).view(1,1)\n",
    "    predicted_weights = hnet.forward(Re_tensor)\n",
    "    predicted_solution = pinn.forward(predicted_weights, inputs)\n",
    "    \n",
    "    f = h5py.File('data/Analytical_Solutions/colocation/Re_{}.h5'.format(Re), 'r')\n",
    "    S_col = f['S_col']\n",
    "    S_col = np.asarray(S_col)\n",
    "    analytical_solution = Variable(torch.from_numpy(S_col).float(), requires_grad=True).to(device)\n",
    "    f.close()\n",
    "    \n",
    "    print(\"Re: \", Re)\n",
    "    # Mean L2 Error\n",
    "    error = torch.linalg.norm(analytical_solution-predicted_solution,2)\n",
    "    print('Mean L2 Error: %.4e' % (error))\n",
    "    # Relative L2 Error\n",
    "    L2_error = torch.linalg.norm(analytical_solution-predicted_solution,2)/torch.linalg.norm(analytical_solution,2)\n",
    "    print('Relative L2 Error: %.4e \\n' % (L2_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
