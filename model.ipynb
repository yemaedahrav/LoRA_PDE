{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameyv\\anaconda3\\envs\\amey-pinn\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.interpolate import griddata\n",
    "from collections import OrderedDict\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Layer-Perceptron(MLP) architecture for our Physics Informed Neural Network(PINN)\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.depth = len(layers) - 1\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1])))\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "        layer_list.append(('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1])))\n",
    "        \n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, xy):\n",
    "        out = self.layers(xy)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the actual PINN containing training and other details\n",
    "class PINN():\n",
    "    def __init__(self, X_bc, Y_bc, S_bc, X, Y, layers, lb, ub, Re):\n",
    "\n",
    "        # Boundary Conditions (Domain Boundaries)\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "\n",
    "        # Boundary Data (Dirichlet Boundary Conditions)\n",
    "        self.X_bc = torch.tensor(X_bc, requires_grad=True).float().to(device)\n",
    "        self.Y_bc = torch.tensor(Y_bc, requires_grad=True).float().to(device)\n",
    "        self.S_bc = torch.tensor(S_bc).float().to(device)\n",
    "        \n",
    "        # Training Data (Colocation Points)\n",
    "        self.X = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        self.Y = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "        \n",
    "        # PINN Hyperparameters\n",
    "        self.layers = layers\n",
    "        self.nu = 1/Re\n",
    "        self.mlp = MLP(layers).to(device)\n",
    "        \n",
    "        # Optimizer (LBFGS)\n",
    "        self.optimizer = torch.optim.LBFGS(self.mlp.parameters(), lr=1, tolerance_grad=1e-5, tolerance_change=1.0 * np.finfo(float).eps, line_search_fn=\"strong_wolfe\")\n",
    "        self.iter = 0\n",
    "        \n",
    "    def netS(self, x, y):\n",
    "        #x = torch.reshape(x, (x.shape[0], 1))\n",
    "        #y = torch.reshape(x, (y.shape[0], 1))\n",
    "        s = self.mlp(torch.cat([x, y], dim=1))\n",
    "        return s\n",
    "    \n",
    "    def netF(self, x, y):\n",
    "        s = self.netS(x, y)\n",
    "        #u = torch.reshape(s[:,0], (s.shape[0], 1))\n",
    "        #v = torch.reshape(s[:,1], (s.shape[0], 1))\n",
    "        #p = torch.reshape(s[:,2], (s.shape[0], 1))\n",
    "        # print(\"u: \", u.shape)\n",
    "        # print(\"u: \", type(u))\n",
    "        # print(\"x: \", x.shape)\n",
    "        # print(\"x: \", type(x))\n",
    "\n",
    "        u = s[:,0]\n",
    "        v = s[:,1]\n",
    "        p = s[:,2]\n",
    "        \n",
    "        du_dy = torch.autograd.grad(inputs=y, outputs=u, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "        dv_dy = torch.autograd.grad(inputs=y, outputs=v, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0]\n",
    "        dp_dy = torch.autograd.grad(inputs=y, outputs=p, grad_outputs=torch.ones_like(p), retain_graph=True, create_graph=True)[0]\n",
    "        \n",
    "        du_dx = torch.autograd.grad(inputs=x, outputs=u, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "        dv_dx = torch.autograd.grad(inputs=x, outputs=v, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0]\n",
    "        dp_dx = torch.autograd.grad(inputs=x, outputs=p, grad_outputs=torch.ones_like(p), retain_graph=True, create_graph=True)[0]\n",
    "        \n",
    "        du_dxx = torch.autograd.grad(inputs=x, outputs=du_dx, grad_outputs=torch.ones_like(du_dx), retain_graph=True, create_graph=True)[0]\n",
    "        du_dyy = torch.autograd.grad(inputs=y, outputs=du_dy, grad_outputs=torch.ones_like(du_dy), retain_graph=True, create_graph=True)[0]\n",
    "        dv_dxx = torch.autograd.grad(inputs=x, outputs=dv_dx, grad_outputs=torch.ones_like(dv_dx), retain_graph=True, create_graph=True)[0]\n",
    "        dv_dyy = torch.autograd.grad(inputs=y, outputs=dv_dy, grad_outputs=torch.ones_like(dv_dy), retain_graph=True, create_graph=True)[0]\n",
    "        \n",
    "        # du_dx, du_dy = torch.autograd.grad(u, (x, y), create_graph=True)\n",
    "        # dv_dx, dv_dy = torch.autograd.grad(v, (x, y), create_graph=True)\n",
    "        # dp_dx, dp_dy = torch.autograd.grad(p, (x, y), create_graph=True)\n",
    "\n",
    "        # du_dxx, du_dxy = torch.autograd.grad(du_dx[0], (x, y))\n",
    "        # du_dyx, du_dyy = torch.autograd.grad(du_dy[1], (x, y))\n",
    "        # dv_dxx, dv_dxy = torch.autograd.grad(dv_dx[0], (x, y))\n",
    "        # dv_dyx, dv_dyy = torch.autograd.grad(dv_dy[1], (x, y))\n",
    "\n",
    "        f1 = u.squeeze() * du_dx + v.squeeze() * du_dy + dp_dx - self.nu * (du_dxx + du_dyy)\n",
    "        f2 = u.squeeze() * dv_dx + v.squeeze() * dv_dy + dp_dy - self.nu * (dv_dxx + dv_dyy)\n",
    "        f = f1 + f2\n",
    "        return f\n",
    "    \n",
    "    def lossFunc(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        S_pred = self.netS(self.X_bc, self.Y_bc)\n",
    "        F_pred = self.netF(self.X, self.Y)\n",
    "\n",
    "        # print(\"F_pred:\", type(F_pred))\n",
    "        # print(\"S_pred:\", type(S_pred))\n",
    "        \n",
    "        S_loss = torch.mean((self.S_bc - S_pred) ** 2)\n",
    "        F_loss = torch.mean(F_pred ** 2)\n",
    "        loss = S_loss + F_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        if self.iter % 5 == 0:\n",
    "            print('Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), S_loss.item(), F_loss.item()))\n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        self.mlp.train()\n",
    "        self.optimizer.step(self.lossFunc)\n",
    "     \n",
    "    def predict(self, X, Y): \n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        y = torch.tensor(Y, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.mlp.eval()\n",
    "        s = self.netS(x, y)\n",
    "        f = self.netF(x, y)\n",
    "        return s, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_bc: (100, 1)\n",
      "Y_bc: (100, 1)\n",
      "S_bc: torch.Size([100, 3])\n",
      "X: (625, 1)\n",
      "Y: (625, 1)\n",
      "Hello\n",
      "Iter 5, Loss: 5.24919e-05, Loss_u: 5.17907e-05, Loss_f: 7.01137e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ameyv\\AppData\\Local\\Temp\\ipykernel_7088\\3750513466.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.S_bc = torch.tensor(S_bc).float().to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10, Loss: 2.88218e-06, Loss_u: 2.12350e-06, Loss_f: 7.58687e-07\n",
      "Iter 15, Loss: 1.34079e-06, Loss_u: 8.16764e-07, Loss_f: 5.24028e-07\n",
      "Iter 20, Loss: 6.35622e-07, Loss_u: 2.88222e-07, Loss_f: 3.47400e-07\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "layers = [2, 20, 20, 20, 20, 20, 3]\n",
    "lb = 0\n",
    "ub = 1\n",
    "Re = 20\n",
    "\n",
    "f = h5py.File('data\\colocation_points.h5', 'r')\n",
    "X = f['X']\n",
    "Y = f['Y']\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "f_bc = h5py.File('data\\dirichlet_boundary_points.h5', 'r')\n",
    "X_bc = f_bc['X']\n",
    "Y_bc = f_bc['Y']\n",
    "X_bc = np.asarray(X_bc)\n",
    "Y_bc = np.asarray(Y_bc)\n",
    "S_bc = torch.zeros(100,3)\n",
    "\n",
    "print(\"X_bc:\", X_bc.shape)\n",
    "print(\"Y_bc:\", Y_bc.shape)\n",
    "print(\"S_bc:\", S_bc.shape)\n",
    "print(\"X:\", X.shape)\n",
    "print(\"Y:\", Y.shape)\n",
    "\n",
    "model = PINN(X_bc, Y_bc, S_bc, X, Y, layers, lb, ub, Re)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "P:  torch.Size([625, 1])\n",
      "U:  torch.Size([625, 1])\n",
      "V:  torch.Size([625, 1])\n",
      "s_pred:  torch.Size([625, 3])\n",
      "s_test:  torch.Size([625, 3])\n",
      "Relative L2 Error y: 1.000221e+00\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('data\\Analytical_Solutions\\Re_20.h5', 'r')\n",
    "#f.visit(printname)\n",
    "P = f['P']\n",
    "U = f['U']\n",
    "V = f['V']\n",
    "P = np.asarray(P)\n",
    "U = np.asarray(U)\n",
    "V = np.asarray(V)\n",
    "U = torch.from_numpy(U)\n",
    "V = torch.from_numpy(V)\n",
    "P = torch.from_numpy(P)\n",
    "print(type(U))\n",
    "print(\"P: \", P.shape)\n",
    "print(\"U: \", U.shape)\n",
    "print(\"V: \", V.shape)\n",
    "f.close()\n",
    "\n",
    "s_test = torch.hstack((U,V,P))\n",
    "s_pred, f_pred = model.predict(X, Y)\n",
    "print(\"s_pred: \", s_pred.shape)\n",
    "print(\"s_test: \", s_test.shape)\n",
    "\n",
    "# Relative L2 Error\n",
    "error_s = torch.linalg.norm(s_test-s_pred,2)/torch.linalg.norm(s_test,2)\n",
    "print('Relative L2 Error y: %e' % (error_s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
