{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce8ab1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e58482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e036835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reynolds numbers from 10 to 90 are used for training \n",
    "# The model is evaluated on the last 10 reynolds numbers from 91 to 100\n",
    "\n",
    "outputs = np.zeros((90-10+1, 368))\n",
    "for Re in range(10,91,1):\n",
    "    outputs[Re-10] = np.loadtxt('pinns/reduced_finetuned/weights/weights_'+str(Re)+'.txt')\n",
    "    \n",
    "# Inputs represent the reynolds numbers used for training\n",
    "# Outputs are the outputs of the hyperLoRA which are the weights of the reduced PINN components (rwo vector and column vector)\n",
    "inputs = np.arange(10,91,1)\n",
    "inputs = np.reshape(inputs, (inputs.shape[0],1))\n",
    "inputs = Variable(torch.from_numpy(inputs).float(), requires_grad=False).to(device)\n",
    "outputs = Variable(torch.from_numpy(outputs).float(), requires_grad=False).to(device)\n",
    "#print(\"inputs\", inputs.shape)\n",
    "#print(\"outputs\", outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98de0e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperLoRA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HyperLoRA, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(nn.Linear(1,512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(512,512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(512,256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(256,256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(256,128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128,368))\n",
    "        \n",
    "    def forward(self, Re):        \n",
    "        weights = self.layers(Re)\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6c044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlora = HyperLoRA()\n",
    "hlora = hlora.to(device)\n",
    "mse_cost_function = nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(hlora.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=20000, gamma=0.1, last_epoch=-1, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc67230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, LR: 1.0000e-03, Loss: 6.8708e-01\n",
      "Epoch 1000, LR: 1.0000e-03, Loss: 1.2385e-01\n",
      "Epoch 2000, LR: 1.0000e-03, Loss: 6.3781e-02\n",
      "Epoch 3000, LR: 1.0000e-03, Loss: 5.2478e-02\n",
      "Epoch 4000, LR: 1.0000e-03, Loss: 5.2267e-02\n",
      "Epoch 5000, LR: 1.0000e-03, Loss: 4.2854e-02\n",
      "Epoch 6000, LR: 1.0000e-03, Loss: 4.6816e-02\n",
      "Epoch 7000, LR: 1.0000e-03, Loss: 4.9659e-02\n",
      "Epoch 8000, LR: 1.0000e-03, Loss: 3.7492e-02\n",
      "Epoch 9000, LR: 1.0000e-03, Loss: 3.3112e-02\n",
      "Epoch 10000, LR: 1.0000e-03, Loss: 3.6131e-02\n",
      "Epoch 11000, LR: 1.0000e-03, Loss: 3.1065e-02\n",
      "Epoch 12000, LR: 1.0000e-03, Loss: 2.5613e-02\n",
      "Epoch 13000, LR: 1.0000e-03, Loss: 3.5260e-02\n",
      "Epoch 14000, LR: 1.0000e-03, Loss: 3.2886e-02\n",
      "Epoch 15000, LR: 1.0000e-03, Loss: 2.7507e-02\n",
      "Epoch 16000, LR: 1.0000e-03, Loss: 2.5256e-02\n",
      "Epoch 17000, LR: 1.0000e-03, Loss: 3.1221e-02\n",
      "Epoch 18000, LR: 1.0000e-03, Loss: 2.7287e-02\n",
      "Epoch 19000, LR: 1.0000e-03, Loss: 2.1886e-02\n",
      "Epoch 20000, LR: 1.0000e-04, Loss: 2.5347e-02\n",
      "Epoch 21000, LR: 1.0000e-04, Loss: 1.7714e-02\n",
      "Epoch 22000, LR: 1.0000e-04, Loss: 1.7284e-02\n",
      "Epoch 23000, LR: 1.0000e-04, Loss: 1.6963e-02\n",
      "Epoch 24000, LR: 1.0000e-04, Loss: 1.6224e-02\n",
      "Epoch 25000, LR: 1.0000e-04, Loss: 1.5845e-02\n",
      "Epoch 26000, LR: 1.0000e-04, Loss: 1.5212e-02\n",
      "Epoch 27000, LR: 1.0000e-04, Loss: 1.4869e-02\n",
      "Epoch 28000, LR: 1.0000e-04, Loss: 1.4371e-02\n",
      "Epoch 29000, LR: 1.0000e-04, Loss: 1.3821e-02\n",
      "Epoch 30000, LR: 1.0000e-04, Loss: 1.3473e-02\n",
      "Epoch 31000, LR: 1.0000e-04, Loss: 1.3283e-02\n",
      "Epoch 32000, LR: 1.0000e-04, Loss: 1.3069e-02\n",
      "Epoch 33000, LR: 1.0000e-04, Loss: 1.3326e-02\n",
      "Epoch 34000, LR: 1.0000e-04, Loss: 1.2703e-02\n",
      "Epoch 35000, LR: 1.0000e-04, Loss: 1.2056e-02\n",
      "Epoch 36000, LR: 1.0000e-04, Loss: 1.1990e-02\n",
      "Epoch 37000, LR: 1.0000e-04, Loss: 1.1597e-02\n",
      "Epoch 38000, LR: 1.0000e-04, Loss: 1.1529e-02\n",
      "Epoch 39000, LR: 1.0000e-04, Loss: 1.1186e-02\n",
      "Epoch 40000, LR: 1.0000e-05, Loss: 1.1282e-02\n",
      "Epoch 41000, LR: 1.0000e-05, Loss: 1.0784e-02\n",
      "Epoch 42000, LR: 1.0000e-05, Loss: 1.0741e-02\n",
      "Epoch 43000, LR: 1.0000e-05, Loss: 1.0693e-02\n",
      "Epoch 44000, LR: 1.0000e-05, Loss: 1.0607e-02\n",
      "Epoch 45000, LR: 1.0000e-05, Loss: 1.0529e-02\n",
      "Epoch 46000, LR: 1.0000e-05, Loss: 1.0447e-02\n",
      "Epoch 47000, LR: 1.0000e-05, Loss: 1.0369e-02\n",
      "Epoch 48000, LR: 1.0000e-05, Loss: 1.0301e-02\n",
      "Epoch 49000, LR: 1.0000e-05, Loss: 1.0252e-02\n",
      "Epoch 50000, LR: 1.0000e-05, Loss: 1.0142e-02\n",
      "Epoch 51000, LR: 1.0000e-05, Loss: 1.0071e-02\n",
      "Epoch 52000, LR: 1.0000e-05, Loss: 9.9991e-03\n",
      "Epoch 53000, LR: 1.0000e-05, Loss: 9.9467e-03\n",
      "Epoch 54000, LR: 1.0000e-05, Loss: 9.8743e-03\n",
      "Epoch 55000, LR: 1.0000e-05, Loss: 9.7958e-03\n",
      "Epoch 56000, LR: 1.0000e-05, Loss: 9.7284e-03\n",
      "Epoch 57000, LR: 1.0000e-05, Loss: 9.6693e-03\n",
      "Epoch 58000, LR: 1.0000e-05, Loss: 9.6095e-03\n",
      "Epoch 59000, LR: 1.0000e-05, Loss: 9.5319e-03\n",
      "Epoch 60000, LR: 1.0000e-06, Loss: 9.4880e-03\n",
      "Epoch 61000, LR: 1.0000e-06, Loss: 9.4522e-03\n",
      "Epoch 62000, LR: 1.0000e-06, Loss: 9.4404e-03\n",
      "Epoch 63000, LR: 1.0000e-06, Loss: 9.4273e-03\n",
      "Epoch 64000, LR: 1.0000e-06, Loss: 9.4115e-03\n",
      "Epoch 65000, LR: 1.0000e-06, Loss: 9.3918e-03\n",
      "Epoch 66000, LR: 1.0000e-06, Loss: 9.3730e-03\n",
      "Epoch 67000, LR: 1.0000e-06, Loss: 9.3545e-03\n",
      "Epoch 68000, LR: 1.0000e-06, Loss: 9.3356e-03\n",
      "Epoch 69000, LR: 1.0000e-06, Loss: 9.3208e-03\n",
      "Epoch 70000, LR: 1.0000e-06, Loss: 9.3001e-03\n",
      "Epoch 71000, LR: 1.0000e-06, Loss: 9.2799e-03\n",
      "Epoch 72000, LR: 1.0000e-06, Loss: 9.2608e-03\n",
      "Epoch 73000, LR: 1.0000e-06, Loss: 9.2448e-03\n",
      "Epoch 74000, LR: 1.0000e-06, Loss: 9.2272e-03\n",
      "Epoch 75000, LR: 1.0000e-06, Loss: 9.2066e-03\n",
      "Epoch 76000, LR: 1.0000e-06, Loss: 9.1890e-03\n",
      "Epoch 77000, LR: 1.0000e-06, Loss: 9.1714e-03\n",
      "Epoch 78000, LR: 1.0000e-06, Loss: 9.1531e-03\n",
      "Epoch 79000, LR: 1.0000e-06, Loss: 9.1385e-03\n",
      "Epoch 80000, LR: 1.0000e-07, Loss: 9.1164e-03\n",
      "Epoch 81000, LR: 1.0000e-07, Loss: 9.1134e-03\n",
      "Epoch 82000, LR: 1.0000e-07, Loss: 9.1111e-03\n",
      "Epoch 83000, LR: 1.0000e-07, Loss: 9.1083e-03\n",
      "Epoch 84000, LR: 1.0000e-07, Loss: 9.1056e-03\n",
      "Epoch 85000, LR: 1.0000e-07, Loss: 9.1027e-03\n",
      "Epoch 86000, LR: 1.0000e-07, Loss: 9.0998e-03\n",
      "Epoch 87000, LR: 1.0000e-07, Loss: 9.0969e-03\n",
      "Epoch 88000, LR: 1.0000e-07, Loss: 9.0939e-03\n",
      "Epoch 89000, LR: 1.0000e-07, Loss: 9.0910e-03\n",
      "Epoch 90000, LR: 1.0000e-07, Loss: 9.0881e-03\n",
      "Epoch 91000, LR: 1.0000e-07, Loss: 9.0852e-03\n",
      "Epoch 92000, LR: 1.0000e-07, Loss: 9.0823e-03\n",
      "Epoch 93000, LR: 1.0000e-07, Loss: 9.0795e-03\n",
      "Epoch 94000, LR: 1.0000e-07, Loss: 9.0769e-03\n",
      "Epoch 95000, LR: 1.0000e-07, Loss: 9.0739e-03\n",
      "Epoch 96000, LR: 1.0000e-07, Loss: 9.0709e-03\n",
      "Epoch 97000, LR: 1.0000e-07, Loss: 9.0681e-03\n",
      "Epoch 98000, LR: 1.0000e-07, Loss: 9.0650e-03\n",
      "Epoch 99000, LR: 1.0000e-07, Loss: 9.0623e-03\n"
     ]
    }
   ],
   "source": [
    "iterations = 100000\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    hlora_out = hlora.forward(inputs)\n",
    "    Loss = mse_cost_function(hlora_out, outputs)\n",
    "    Loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "        if epoch%1000 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print('Epoch %d, LR: %.4e, Loss: %.4e' % (epoch, current_lr, Loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba9971ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "    \n",
    "    def forward(self, weights, vectors, x):\n",
    "        W = weights[0, :40].view(20, 2)\n",
    "        A = vectors[0, :20].view(20,1)\n",
    "        B = vectors[0, 20:22].view(1,2)\n",
    "        cur_weight = torch.add(W, torch.mm(A,B))\n",
    "        h = torch.tanh(F.linear(x, weight = cur_weight, bias = vectors[0, 22:42].view(20)))\n",
    "        i = 60\n",
    "        j = 42\n",
    "        for _ in range(5):\n",
    "            W = weights[0, i:i+400].view(20, 20)\n",
    "            A = vectors[0, j:j+20].view(20,1)\n",
    "            B = vectors[0, j+20:j+40].view(1,20)\n",
    "            cur_weight = torch.add(W, torch.mm(A,B))\n",
    "            h = torch.tanh(F.linear(h, weight = cur_weight, bias = vectors[0, j+40:j+60].view(20)))\n",
    "            i += 420\n",
    "            j+=60\n",
    "        W = weights[0, i:i+60].view(3, 20)\n",
    "        A = vectors[0, j:j+3].view(3,1)\n",
    "        B = vectors[0, j+3:j+23].view(1,20)\n",
    "        cur_weight = torch.add(W, torch.mm(A,B))\n",
    "        h = F.linear(h, weight = cur_weight, bias = vectors[0, j+23:j+26].view(3))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a05799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re:  91\n",
      "Mean L2 Error: 2.7430e+02\n",
      "Relative L2 Error: 4.5764e+00 \n",
      "\n",
      "Re:  92\n",
      "Mean L2 Error: 2.7299e+02\n",
      "Relative L2 Error: 4.5540e+00 \n",
      "\n",
      "Re:  93\n",
      "Mean L2 Error: 2.7167e+02\n",
      "Relative L2 Error: 4.5315e+00 \n",
      "\n",
      "Re:  94\n",
      "Mean L2 Error: 2.7034e+02\n",
      "Relative L2 Error: 4.5087e+00 \n",
      "\n",
      "Re:  95\n",
      "Mean L2 Error: 2.6900e+02\n",
      "Relative L2 Error: 4.4860e+00 \n",
      "\n",
      "Re:  96\n",
      "Mean L2 Error: 2.6777e+02\n",
      "Relative L2 Error: 4.4650e+00 \n",
      "\n",
      "Re:  97\n",
      "Mean L2 Error: 2.6662e+02\n",
      "Relative L2 Error: 4.4453e+00 \n",
      "\n",
      "Re:  98\n",
      "Mean L2 Error: 2.6557e+02\n",
      "Relative L2 Error: 4.4273e+00 \n",
      "\n",
      "Re:  99\n",
      "Mean L2 Error: 2.6460e+02\n",
      "Relative L2 Error: 4.4107e+00 \n",
      "\n",
      "Re:  100\n",
      "Mean L2 Error: 2.6371e+02\n",
      "Relative L2 Error: 4.3954e+00 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pinn = PINN()\n",
    "\n",
    "f = h5py.File('data/colocation_points.h5', 'r')\n",
    "X_col = f['X_col']\n",
    "Y_col = f['Y_col']\n",
    "X_col = np.asarray(X_col)\n",
    "Y_col = np.asarray(Y_col)\n",
    "X_col = Variable(torch.from_numpy(X_col).float(), requires_grad=True).to(device)\n",
    "Y_col = Variable(torch.from_numpy(Y_col).float(), requires_grad=True).to(device)\n",
    "inputs = torch.cat([X_col, Y_col], axis=1)\n",
    "f.close()\n",
    "\n",
    "pretrained_weights = np.loadtxt('pinns/pretrained/weights/weights_'+str(Re)+'.txt')\n",
    "pretrained_weights = torch.tensor(pretrained_weights, dtype=torch.float32).view(1, pretrained_weights.size)\n",
    "#print(\"weights: \", pretrained_weights.shape)\n",
    "\n",
    "for Re in range(91,101,1):\n",
    "    Re_tensor = torch.tensor(Re, dtype=torch.float32).view(1,1)\n",
    "    predicted_vectors = hlora.forward(Re_tensor)\n",
    "    #print(\"vectors: \", predicted_vectors.shape)\n",
    "    predicted_solution = pinn.forward(pretrained_weights, predicted_vectors, inputs)\n",
    "    \n",
    "    f = h5py.File('data/Analytical_Solutions/colocation/Re_{}.h5'.format(Re), 'r')\n",
    "    S_col = f['S_col']\n",
    "    S_col = np.asarray(S_col)\n",
    "    analytical_solution = Variable(torch.from_numpy(S_col).float(), requires_grad=True).to(device)\n",
    "    f.close()\n",
    "    \n",
    "    print(\"Re: \", Re)\n",
    "    # Mean L2 Error\n",
    "    error = torch.linalg.norm(analytical_solution-predicted_solution,2)\n",
    "    print('Mean L2 Error: %.4e' % (error))\n",
    "    # Relative L2 Error\n",
    "    L2_error = torch.linalg.norm(analytical_solution-predicted_solution,2)/torch.linalg.norm(analytical_solution,2)\n",
    "    print('Relative L2 Error: %.4e \\n' % (L2_error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
